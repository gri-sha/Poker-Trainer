{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here the algorithm of **regret matching** for Rock-Paper-Scissors to find the best response strategy to the strategy of the opponent is released"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROCK, PAPER, SCISSORS = 0, 1, 2\n",
    "NUM_ACTIONS = 3\n",
    "\n",
    "# Compute action utilities using utility matrix\n",
    "# myAction - choose row, otherAction - choose col, RPS accordingly to the order\n",
    "actionUtility = [[0, -1, 1],\n",
    "                    [1, 0, -1],\n",
    "                    [-1, 1, 0]]\n",
    "\n",
    "regretSum = [0] * NUM_ACTIONS\n",
    "strategy = [0] * NUM_ACTIONS\n",
    "strategySum = [0] * NUM_ACTIONS\n",
    "\n",
    "# Choose a strategy for the opponent\n",
    "oppStrategy = [0.4, 0.3, 0.3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to get current mixed strategy through regret-matching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_strategy():\n",
    "    normalizing_sum = 0\n",
    "    for action in range(NUM_ACTIONS):\n",
    "        strategy[action] = max(0, regretSum[action])\n",
    "        normalizing_sum += strategy[action]\n",
    "\n",
    "    for action in range(NUM_ACTIONS):\n",
    "        if normalizing_sum > 0:\n",
    "            strategy[action] /= normalizing_sum\n",
    "        else:\n",
    "            # If we don't regret about anything\n",
    "            strategy[action] = 1.0 / NUM_ACTIONS\n",
    "        strategySum[action] += strategy[action]\n",
    "\n",
    "    return strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to get random action according to mixed-strategy distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(strategy):\n",
    "    # Next time for this function numpy will be used\n",
    "    r = random.random()\n",
    "    action = 0\n",
    "    cumulativeProbability = 0\n",
    "    while action < NUM_ACTIONS - 1:\n",
    "        cumulativeProbability += strategy[action]\n",
    "        if r < cumulativeProbability:\n",
    "            break\n",
    "        action += 1\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to get average mixed strategy across all training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_strategy():\n",
    "    avg_strategy = [0.0] * NUM_ACTIONS\n",
    "    normalizing_sum = sum(strategySum)\n",
    "    for action in range(NUM_ACTIONS):\n",
    "        if normalizing_sum > 0:\n",
    "            avg_strategy[action] = strategySum[action] / normalizing_sum\n",
    "        else:\n",
    "            avg_strategy[action] = 1.0 / NUM_ACTIONS\n",
    "    return avg_strategy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these building blocks in place, we can now construct our training algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(iterations):\n",
    "    for _ in range(iterations):\n",
    "\n",
    "        # Get regret-matched mixed strategy actions\n",
    "        strategy = get_strategy()\n",
    "        myAction = get_action(strategy)\n",
    "        otherAction = get_action(oppStrategy)\n",
    "\n",
    "        # Accumulate action regrets\n",
    "        for action in range(NUM_ACTIONS):\n",
    "            # Case if we consider negative regrets\n",
    "            regretSum[action] += (actionUtility[action][otherAction] - actionUtility[myAction][otherAction])\n",
    "\n",
    "            # Case if we don't consider negative regrets\n",
    "            # regretSum[action] += max(0 , actionUtility[action][otherAction] - actionUtility[myAction][otherAction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to run the computation of the response strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00022815968170315993, 0.9993322188194254, 0.0004396214988714989]\n"
     ]
    }
   ],
   "source": [
    "# Obtained strategy converges to [0, 1, 0] if we consider negative regrets\n",
    "# And to ~ [0.33, 0,36, 0,31] if we don't\n",
    "train(iterations=100_000)\n",
    "print(get_average_strategy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Expected utility if we consider positive and negative regrets: 0.099\n",
      " Expected utility if we consider only positive regrets: 0.005\n"
     ]
    }
   ],
   "source": [
    "# Comparison of these two cases\n",
    "\n",
    "strategyPN = [0.0022815968170315994, 0.9933221881942534, 0.004396214988714989]\n",
    "strategyP = [0.32656612190646006, 0.36233503087561797, 0.31109884721792197]\n",
    "\n",
    "expUtilityPN = 0\n",
    "expUtilityP = 0\n",
    "\n",
    "for i in range(NUM_ACTIONS):\n",
    "    for j in range(NUM_ACTIONS):\n",
    "        expUtilityPN += strategyPN[i] * oppStrategy[j] * actionUtility[i][j]\n",
    "        expUtilityP += strategyP[i] * oppStrategy[j] * actionUtility[i][j]\n",
    "        \n",
    "print(f' Expected utility if we consider positive and negative regrets: {expUtilityPN:0.3f}')\n",
    "print(f' Expected utility if we consider only positive regrets: {expUtilityP:0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can conclude that it is better to consider wins and losses for regret minimization, because the expected utility will be higher"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
